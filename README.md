# Disaster Tweet Classification-Kaggle Competition

#### Problem Statement
Smartphones are owned by almost anyone these days which has in turn caused an explosive rise in use of social media over the past few decades. People are constantly communicating their opinions on current events, personal life occurences, internet memes, and observations in the real world. This constant stream of information while diversionary and insightful to human minds are often most peoples primary source of important alerts in the happenings of the world. Twitter for example, is a vast and constantly updating source of information and method of communication that has been avoided as a source of information due to scraping and readability issues based off the way that people typically type and difficult of accessing other metrics such as location. However, with advances in NLP processing and modelling, social media can be used to read tweets and even understand whether a person is tweeting about normal non disaster related events, or emergencies that can be used to generate and put out alerts. This project will show that social media is a valid source of information even with simple modelling techniques.<br>

#### Executive Summary

Data was taken from the [Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started/data) kaggle competition where 10,000 tweets were hand classified between tweets pertinent to disasters and tweets not disaster related.<br>

Tweets from the dataset were cleaned of links, nltk stop words,non-alphabetical characters, converted to lowercase, and lemmatized.
They were then vectorized using a tfidf vectorizer which is able to judge the importance of each individual word in a tweet not only by its frequency of appearance in a tweet, but also its appearence in all tweets observed. The tfidf vectorized data set and also a count vectorized data set was then fed into a gridsearch which compared the prediction performance of a random forest classifer, adaboost classifer, and a support vector machine for classification using a range of monogram, bigram, and trigram ranges, kernels for the support vector machines. The tfidf vectorized support vector machine yielded the highest scores when validated with accuracy scores and an f1-score meant to take into account the precision and recall metrics of the model as well. The final model that yielded the peak f1-score of 0.77 and an accuracy of 81% (outperforming the baseline scores: .601, and 57% respectively) used a linear kernel, an ngram range of (1,2), and no stop words or regularization.

#### Conclusion
The final model is a relatively simple one. Even with the default parameters and basic modeling techniques, the model was able to classify over 80% of the tweets in the testing corpus correctly. Using more updated machine learning techniques such as recurrent neural networks or even another type of vectorizer such as a word2vec or doc2vec model, the model accuracy and sensitivity could be optimized significantly. Social media is a tedious source of data to be able to scrape useful information from. The data acquired for this notebook was graciously orgainized and hand classified by the company [Figure-Eight](https://www.figure-eight.com/data-for-everyone/). However, with advancements in technology and the quickly increasing availability of data, social media is becoming a more and more accessible source for data that can be used in numerous applications beneficial to society. This model that is able to distinguish between disaster related and non-disaster related tweets could be used to map the occurence of road closures, and natural disasters if location data was more readily available in order to map evacuation routes even when satellite imaging is not able to get accurate and updated information. Next steps to improving this model would be to read and infer locations using the text information in the tweets and possibly develop a map using latitude and longitude coordinates derived from these inferences. 
